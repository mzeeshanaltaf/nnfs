{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4180a5c5-6976-497f-82db-7978d76e8f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nnfs.datasets import spiral_data\n",
    "import nnfs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "329525cb-3943-4426-91bc-a7223759da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons, weight_regularizer_l1=0, weight_regularizer_l2=0, bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599280c-0071-4e15-8635-5bd014a39a0a",
   "metadata": {},
   "source": [
    "## Regularization Loss Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e68ba3e-dc51-4656-97d2-8e87511785d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self, layer):\n",
    "        regularization_loss = 0\n",
    "        # L1 & L2 regularizaiton - weights. Calculate only when regularization factor is > 0\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "        \n",
    "        # L1 & L2 regularizaiton - biases. Calculate only when regularization factor is > 0\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a39ceb-3d1d-4d41-9b09-8067c646bafc",
   "metadata": {},
   "source": [
    "### Backward Pass with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e8cbced-fb4d-4ca0-a667-71f2f650fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    # Backward Pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights <0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        \n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "        \n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases <0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        \n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3326c90-7e0e-4a58-9a32-351419f1a230",
   "metadata": {},
   "source": [
    "### Creating a Layer and Loss Class with Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe532f6-3ecb-4e8d-9c17-e001e36f5360",
   "metadata": {},
   "source": [
    "### Complete Layer Dense Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2b28eca-4686-4bd8-b2c7-6f69d9e397cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    \n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons, weight_regularizer_l1=0, weight_regularizer_l2=0, bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "    # Forward Pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs # Remeber input values\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases # Calculate output fom input, weights and biases\n",
    "        \n",
    "    # Backward Pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights <0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "        \n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases <0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        \n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a093b-b9b2-4f6d-9421-fbd7b34f0287",
   "metadata": {},
   "source": [
    "### Complete Loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a4e2cc2-644d-4a91-9c59-d48897a1001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self, layer):\n",
    "        regularization_loss = 0\n",
    "        # L1 & L2 regularizaiton - weights. Calculate only when regularization factor is > 0\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "        \n",
    "        # L1 & L2 regularizaiton - biases. Calculate only when regularization factor is > 0\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "    \n",
    "    def calculate(self, output, y): # Calculates the data and regularization losses given model o/p and ground truth values\n",
    "        sample_losses = self.forward(output, y) # Calculate the sample losses\n",
    "        data_loss = np.mean(sample_losses) # Calculate the mean loss\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195fa3b3-4847-45d4-a8c8-dc2a7e7105f0",
   "metadata": {},
   "source": [
    "### ReLU Activaiton Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "23bfc2d9-3786-4e63-b6ba-cb8265269c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    # Forward Pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    # Backward Pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify the original variable, make a copy of the values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Gradient to be 0 if input values are 0 or negative\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce39323-48b2-41ec-a2aa-c3990176e652",
   "metadata": {},
   "source": [
    "## Softmax Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "063e9c98-7751-485b-af1f-65c153f31a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2b2153-1b6a-4c2f-ad70-5d8c0423d7a1",
   "metadata": {},
   "source": [
    "## Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e1b351d2-69ec-48d1-8b19-7f5aa1ef4b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):        \n",
    "        samples = len(y_pred)  # Number of samples in a batch\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) # Clip data to prevent division by 0\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_onfidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_onfidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        \n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_onfidences)\n",
    "        return negative_log_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdc85b5-e561-4bdb-830e-3de4086e46b4",
   "metadata": {},
   "source": [
    "## Combined Softmax activation and cross-entropy loss for faster backward step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da152d1a-5dc6-4d61-aaec-c273e32ef993",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossEntropy:\n",
    "    # Create activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    # Backward Pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded, turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate the gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize the gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93875228-6eb1-4018-8b6c-952208a53996",
   "metadata": {},
   "source": [
    "## Optimizer -- ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f4f12fac-06a7-4110-9f2a-12a800b3dd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate=0.001, decay=0, epsilon=1e-7, beta_1=0.9, beta_2=0.999):  # Learning rate of 1 is default for this optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays, create them and filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # Update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums =   self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "        \n",
    "        # Get corrected momentum. Adding 1 in self.iterations as it starts from 0\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        # Update Cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "\t\n",
    "        # Parameter update  + normalization with square root of cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    # Call once after the parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67494877-7ddf-4dcd-8aaf-827c3a75732b",
   "metadata": {},
   "source": [
    "## Training a simple NN with and without Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7532ff4-1229-4f03-a3b0-ded39d937472",
   "metadata": {},
   "source": [
    "### Without Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb8eff69-afa2-48a2-b1f9-ac3bbf65aa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Accuracy: 0.343, Loss: 1.099 Data Loss: 1.099 Regularizatio Loss: 0.000 Learning Rate: 0.020\n",
      "Epoch: 1000, Accuracy: 0.857, Loss: 0.375 Data Loss: 0.375 Regularizatio Loss: 0.000 Learning Rate: 0.020\n",
      "Epoch: 2000, Accuracy: 0.900, Loss: 0.252 Data Loss: 0.252 Regularizatio Loss: 0.000 Learning Rate: 0.020\n",
      "Epoch: 3000, Accuracy: 0.917, Loss: 0.209 Data Loss: 0.209 Regularizatio Loss: 0.000 Learning Rate: 0.020\n",
      "Epoch: 4000, Accuracy: 0.913, Loss: 0.188 Data Loss: 0.188 Regularizatio Loss: 0.000 Learning Rate: 0.020\n",
      "Epoch: 5000, Accuracy: 0.923, Loss: 0.172 Data Loss: 0.172 Regularizatio Loss: 0.000 Learning Rate: 0.020\n",
      "Epoch: 6000, Accuracy: 0.923, Loss: 0.160 Data Loss: 0.160 Regularizatio Loss: 0.000 Learning Rate: 0.020\n",
      "Epoch: 7000, Accuracy: 0.933, Loss: 0.152 Data Loss: 0.152 Regularizatio Loss: 0.000 Learning Rate: 0.020\n",
      "Epoch: 8000, Accuracy: 0.940, Loss: 0.145 Data Loss: 0.145 Regularizatio Loss: 0.000 Learning Rate: 0.020\n",
      "Epoch: 9000, Accuracy: 0.940, Loss: 0.143 Data Loss: 0.143 Regularizatio Loss: 0.000 Learning Rate: 0.020\n",
      "Epoch: 10000, Accuracy: 0.943, Loss: 0.134 Data Loss: 0.134 Regularizatio Loss: 0.000 Learning Rate: 0.020\n",
      "Validation Accuracy: 0.783, Loss :0.950\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense Layer with 2 inputs features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create 2nd Dense layer with 64 input features (previous layer has 64 outputs) and 3 output values (spiral dataset has 3 categories)\n",
    "dense2 = Layer_Dense(64,3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# Create Optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.02, decay=5e-7)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Forward Pass through first layer\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # Forward Pass through second layer\n",
    "    dense2.forward(activation1.output)\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 1000:\n",
    "        print(f\"Epoch: {epoch}, \" + f\"Accuracy: {accuracy:.3f}, \" + f\"Loss: {loss:.3f} \"  + f\"Data Loss: {data_loss:.3f} \"  + \n",
    "              f\"Regularizatio Loss: {regularization_loss:.3f} \" + f\"Learning Rate: {optimizer.current_learning_rate:.3f}\")\n",
    "    \n",
    "    # Backward Pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "# Validate the model\n",
    "# Create the test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f'Validation Accuracy: {accuracy:.3f}, Loss :{loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22b0ff5-3374-4928-bb3c-a8ce272d1132",
   "metadata": {},
   "source": [
    "### With Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b2e0db00-6772-423e-bbda-a8d609476f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Accuracy: 0.340, Loss: 1.099 Data Loss: 1.099 Regularizatio Loss: 0.000 Learning Rate: 0.020\n",
      "Epoch: 1000, Accuracy: 0.860, Loss: 0.466 Data Loss: 0.371 Regularizatio Loss: 0.095 Learning Rate: 0.020\n",
      "Epoch: 2000, Accuracy: 0.867, Loss: 0.397 Data Loss: 0.307 Regularizatio Loss: 0.090 Learning Rate: 0.020\n",
      "Epoch: 3000, Accuracy: 0.913, Loss: 0.346 Data Loss: 0.263 Regularizatio Loss: 0.083 Learning Rate: 0.020\n",
      "Epoch: 4000, Accuracy: 0.920, Loss: 0.320 Data Loss: 0.243 Regularizatio Loss: 0.077 Learning Rate: 0.020\n",
      "Epoch: 5000, Accuracy: 0.930, Loss: 0.305 Data Loss: 0.229 Regularizatio Loss: 0.076 Learning Rate: 0.020\n",
      "Epoch: 6000, Accuracy: 0.933, Loss: 0.288 Data Loss: 0.216 Regularizatio Loss: 0.072 Learning Rate: 0.020\n",
      "Epoch: 7000, Accuracy: 0.923, Loss: 0.272 Data Loss: 0.204 Regularizatio Loss: 0.068 Learning Rate: 0.020\n",
      "Epoch: 8000, Accuracy: 0.933, Loss: 0.261 Data Loss: 0.194 Regularizatio Loss: 0.067 Learning Rate: 0.020\n",
      "Epoch: 9000, Accuracy: 0.937, Loss: 0.251 Data Loss: 0.187 Regularizatio Loss: 0.064 Learning Rate: 0.020\n",
      "Epoch: 10000, Accuracy: 0.940, Loss: 0.241 Data Loss: 0.179 Regularizatio Loss: 0.062 Learning Rate: 0.020\n",
      "Validation Accuracy: 0.837, Loss :0.541\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense Layer with 2 inputs features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
    "\n",
    "# Create ReLU activation\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create 2nd Dense layer with 64 input features (previous layer has 64 outputs) and 3 output values (spiral dataset has 3 categories)\n",
    "dense2 = Layer_Dense(64,3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# Create Optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.02, decay=5e-7)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Forward Pass through first layer\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # Forward Pass through second layer\n",
    "    dense2.forward(activation1.output)\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 1000:\n",
    "        print(f\"Epoch: {epoch}, \" + f\"Accuracy: {accuracy:.3f}, \" + f\"Loss: {loss:.3f} \"  + f\"Data Loss: {data_loss:.3f} \"  + \n",
    "              f\"Regularizatio Loss: {regularization_loss:.3f} \" + f\"Learning Rate: {optimizer.current_learning_rate:.3f}\")\n",
    "    \n",
    "    # Backward Pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "# Validate the model\n",
    "# Create the test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f'Validation Accuracy: {accuracy:.3f}, Loss :{loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b138353-d2cc-4ab9-b174-ac892315c72a",
   "metadata": {},
   "source": [
    "## Effect of More Training Data with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0fa06419-e531-4132-b180-633546e20004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Accuracy: 0.335, Loss: 1.099 Data Loss: 1.099 Regularizatio Loss: 0.000 Learning Rate: 0.020\n",
      "Epoch: 1000, Accuracy: 0.860, Loss: 0.503 Data Loss: 0.409 Regularizatio Loss: 0.094 Learning Rate: 0.020\n",
      "Epoch: 2000, Accuracy: 0.872, Loss: 0.430 Data Loss: 0.349 Regularizatio Loss: 0.081 Learning Rate: 0.020\n",
      "Epoch: 3000, Accuracy: 0.878, Loss: 0.399 Data Loss: 0.327 Regularizatio Loss: 0.072 Learning Rate: 0.020\n",
      "Epoch: 4000, Accuracy: 0.885, Loss: 0.373 Data Loss: 0.306 Regularizatio Loss: 0.066 Learning Rate: 0.020\n",
      "Epoch: 5000, Accuracy: 0.889, Loss: 0.358 Data Loss: 0.296 Regularizatio Loss: 0.062 Learning Rate: 0.020\n",
      "Epoch: 6000, Accuracy: 0.888, Loss: 0.345 Data Loss: 0.287 Regularizatio Loss: 0.057 Learning Rate: 0.020\n",
      "Epoch: 7000, Accuracy: 0.887, Loss: 0.336 Data Loss: 0.283 Regularizatio Loss: 0.053 Learning Rate: 0.020\n",
      "Epoch: 8000, Accuracy: 0.890, Loss: 0.328 Data Loss: 0.278 Regularizatio Loss: 0.050 Learning Rate: 0.020\n",
      "Epoch: 9000, Accuracy: 0.890, Loss: 0.323 Data Loss: 0.276 Regularizatio Loss: 0.048 Learning Rate: 0.020\n",
      "Epoch: 10000, Accuracy: 0.893, Loss: 0.318 Data Loss: 0.272 Regularizatio Loss: 0.045 Learning Rate: 0.020\n",
      "Validation Accuracy: 0.867, Loss :0.302\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "X, y = spiral_data(samples=1000, classes=3)\n",
    "\n",
    "# Create Dense Layer with 2 inputs features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
    "\n",
    "# Create ReLU activation\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create 2nd Dense layer with 64 input features (previous layer has 64 outputs) and 3 output values (spiral dataset has 3 categories)\n",
    "dense2 = Layer_Dense(64,3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# Create Optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.02, decay=5e-7)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Forward Pass through first layer\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # Forward Pass through second layer\n",
    "    dense2.forward(activation1.output)\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 1000:\n",
    "        print(f\"Epoch: {epoch}, \" + f\"Accuracy: {accuracy:.3f}, \" + f\"Loss: {loss:.3f} \"  + f\"Data Loss: {data_loss:.3f} \"  + \n",
    "              f\"Regularizatio Loss: {regularization_loss:.3f} \" + f\"Learning Rate: {optimizer.current_learning_rate:.3f}\")\n",
    "    \n",
    "    # Backward Pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "# Validate the model\n",
    "# Create the test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f'Validation Accuracy: {accuracy:.3f}, Loss :{loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "59a95e40-2976-4be3-87f1-160375b14204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2, 3, 2, 2, 3, 3, 3, 2])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.binomial(3, 0.8, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7430d160-d4a7-4a7c-ba60-48282deb4321",
   "metadata": {},
   "source": [
    "## Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cbb38de5-89f0-4a1b-91eb-06bf8814cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dropout:\n",
    "    # Initialize the dropout layer\n",
    "    def __init__(self, rate):\n",
    "        # Store the dropout rate. Invert it to get the success rate\n",
    "        # For example, for a dropout of 0.1, we need a success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs # Save the input values\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate # Generate and save the scaled binary mask\n",
    "        self.output = inputs * self.binary_mask # Apply mask to output values\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1b3181c3-bd1f-47f8-b5e6-cd8c059c159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Accuracy: 0.390, Loss: 1.099 Data Loss: 1.099 Regularizatio Loss: 0.000 Learning Rate: 0.050\n",
      "Epoch: 1000, Accuracy: 0.690, Loss: 0.721 Data Loss: 0.672 Regularizatio Loss: 0.049 Learning Rate: 0.048\n",
      "Epoch: 2000, Accuracy: 0.700, Loss: 0.705 Data Loss: 0.657 Regularizatio Loss: 0.048 Learning Rate: 0.045\n",
      "Epoch: 3000, Accuracy: 0.700, Loss: 0.697 Data Loss: 0.648 Regularizatio Loss: 0.048 Learning Rate: 0.043\n",
      "Epoch: 4000, Accuracy: 0.770, Loss: 0.678 Data Loss: 0.628 Regularizatio Loss: 0.049 Learning Rate: 0.042\n",
      "Epoch: 5000, Accuracy: 0.740, Loss: 0.683 Data Loss: 0.634 Regularizatio Loss: 0.048 Learning Rate: 0.040\n",
      "Epoch: 6000, Accuracy: 0.743, Loss: 0.631 Data Loss: 0.581 Regularizatio Loss: 0.050 Learning Rate: 0.038\n",
      "Epoch: 7000, Accuracy: 0.747, Loss: 0.608 Data Loss: 0.557 Regularizatio Loss: 0.051 Learning Rate: 0.037\n",
      "Epoch: 8000, Accuracy: 0.753, Loss: 0.603 Data Loss: 0.550 Regularizatio Loss: 0.052 Learning Rate: 0.036\n",
      "Epoch: 9000, Accuracy: 0.733, Loss: 0.611 Data Loss: 0.560 Regularizatio Loss: 0.052 Learning Rate: 0.034\n",
      "Epoch: 10000, Accuracy: 0.733, Loss: 0.714 Data Loss: 0.662 Regularizatio Loss: 0.052 Learning Rate: 0.033\n",
      "Validation Accuracy: 0.733, Loss :0.743\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense Layer with 2 inputs features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
    "\n",
    "# Create ReLU activation\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create a dropout layer\n",
    "dropout1 = Layer_Dropout(0.1)\n",
    "\n",
    "# Create 2nd Dense layer with 64 input features (previous layer has 64 outputs) and 3 output values (spiral dataset has 3 categories)\n",
    "dense2 = Layer_Dense(64,3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# Create Optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-5)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Forward Pass through first layer\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through Dropout layer\n",
    "    dropout1.forward(activation1.output)\n",
    "    \n",
    "    # Forward Pass through second layer\n",
    "    dense2.forward(dropout1.output)\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 1000:\n",
    "        print(f\"Epoch: {epoch}, \" + f\"Accuracy: {accuracy:.3f}, \" + f\"Loss: {loss:.3f} \"  + f\"Data Loss: {data_loss:.3f} \"  + \n",
    "              f\"Regularizatio Loss: {regularization_loss:.3f} \" + f\"Learning Rate: {optimizer.current_learning_rate:.3f}\")\n",
    "    \n",
    "    # Backward Pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    dropout1.backward(dense2.dinputs)\n",
    "    activation1.backward(dropout1.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "# Validate the model\n",
    "# Create the test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f'Validation Accuracy: {accuracy:.3f}, Loss :{loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fef82d-2439-49fb-aa0a-ead39ee86b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18cf9f-948a-4281-9c00-70d08c0bc0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b41190-0cc3-40ea-94af-ee41b253e23a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
